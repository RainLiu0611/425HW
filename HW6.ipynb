{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6011d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1787b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d560e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the true data\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate n data points from N(0, 1)\n",
    "n = 1000000\n",
    "x = norm.rvs(size=n)\n",
    "\n",
    "# Compute corresponding y values using the true data generating process\n",
    "sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "y_true = sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b89f61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the 1-layer and 2-layer neural network model\n",
    "\n",
    "# Define the 1-layer neural network model\n",
    "def nn_1_layer(weights, inputs):\n",
    "    return sigmoid(weights[0] * inputs)\n",
    "\n",
    "# Define the loss function for the 1-layer NN\n",
    "def loss_function_1_layer(weights, inputs, true_outputs):\n",
    "    predictions = nn_1_layer(weights, inputs)\n",
    "    return np.mean((predictions - true_outputs) ** 2)\n",
    "\n",
    "# Define the 2-layer neural network model with identity activation for the output layer\n",
    "def nn_2_layer_identity(weights, inputs):\n",
    "    h1 = sigmoid(weights[0] * inputs)  # Output of the first hidden layer\n",
    "    h2 = sigmoid(weights[1] * h1)      # Output of the second hidden layer\n",
    "    y_hat = weights[2] * h2            # Identity activation for the output layer\n",
    "    return y_hat\n",
    "\n",
    "# Define the loss function for the 2-layer NN with identity activation at the output\n",
    "def loss_function_2_layer_identity(weights, inputs, true_outputs):\n",
    "    predictions = nn_2_layer_identity(weights, inputs)\n",
    "    return np.mean((predictions - true_outputs) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95120120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model with the true data\n",
    "\n",
    "# Initial guess for weights\n",
    "initial_weights_1_layer = np.array([0.1])\n",
    "initial_weights_2_layer_identity = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "# Optimize the 1-layer neural network\n",
    "res_1_layer = minimize(loss_function_1_layer, initial_weights_1_layer, args=(x, y_true))\n",
    "\n",
    "# Optimize the 2-layer neural network with identity activation at the output\n",
    "res_2_layer_identity = minimize(loss_function_2_layer_identity, initial_weights_2_layer_identity, args=(x, y_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135a8d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Layer NN - Training error: 1.5217977971179198e-11, Optimized weight: [0.99997467]\n",
      "2-Layer NN - Training error: 0.010075646134338302, Optimized weights: [5.59336842 3.46053154 0.67314477]\n"
     ]
    }
   ],
   "source": [
    "# Results for the 1-layer NN\n",
    "training_error_1_layer = res_1_layer.fun\n",
    "optimized_weights_1_layer = res_1_layer.x\n",
    "\n",
    "# Results for the 2-layer NN with identity activation at the output(here we get the errors and the optimum weights of model\n",
    "#through the gradient process)\n",
    "training_error_2_layer_identity = res_2_layer_identity.fun\n",
    "optimized_weights_2_layer_identity = res_2_layer_identity.x\n",
    "\n",
    "# Print the results for both models\n",
    "print(f\"1-Layer NN - Training error: {training_error_1_layer}, Optimized weight: {optimized_weights_1_layer}\")\n",
    "print(f\"2-Layer NN - Training error: {training_error_2_layer_identity}, Optimized weights: {optimized_weights_2_layer_identity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4ff5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reason why adding an layer actually makes the testing error larger is because of the added complexity of the model. \n",
    "#When the true function can be modeled correctly with a simpler model(which is only one sigmoid function),\n",
    "#adding more layers may lead to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2df13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
